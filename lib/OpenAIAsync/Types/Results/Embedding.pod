=pod

=head1 NAME

OpenAIAsync::Types::Results::Completion

=head1 DESCRIPTION

A result from a an embedding request, L<OpenAIAsync::Types::Request::Completion>

=head1 SYNOPSIS

  use IO::Async::Loop;
  use OpenAIAsync::Client;

  my $loop = IO::Async::Loop->new();
  my $client = OpenAIAsync::Client->new();

  $loop->add($client);

  my $output = $client->embedding({
      input => "My hovercraft is full of eels",
      model => "text-embedding-ada-002",
      encoding_format => "float"
  })->get();

  print Dumper($output->data->embedding);

=head1 Fields

=head2 model

The model that was used to generate the response.  Usually will be what you requested,
but some local inference servers will ignore what was requested and use the model that was
already loaded, and this will reflect what was loaded.

=head2 data

An C<OpenAIAsync::Types::Results::EmbeddingData> object, used just for this

it has the following fields: C<index>, C<embedding>, C<object>

Of these, you probably only want embedding as it's the list of the numbers representing the embedding vector

=head2 usage

A L<OpenAIAsync::Tupes::Results::Usage> object, has three fields C<total_tokens>, C<prompt_tokens>, and C<completion_tokens>

=head2 object

A string describing what kind of result this was, will always be "compleembeddingtion".

=head1 SEE ALSO

L<OpenAIAsync::Types::Request::Completion>, L<OpenAIAsync::Client>

=head1 AUTHOR

Ryan Voots ...

=cut